{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUnNlCJp7Bbg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numba\n",
        "from numba import jit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(numba.__version__)\n",
        "scale = 1e-3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol09-UatODS_",
        "outputId": "7b20d738-c8b8-4b50-b0e8-9177b2f9b6a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.56.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDM7Ttet7woD"
      },
      "outputs": [],
      "source": [
        "########################################################\n",
        "## Convolution Operation\n",
        "## Optimized using NP einsum\n",
        "## Reference: https://ajcr.net/Basic-guide-to-einsum/\n",
        "########################################################\n",
        "\n",
        "## Function to get Windows \n",
        "\n",
        "def fetch_windows(input: np.ndarray, conv_dim, kernel_size, padding=0):\n",
        "    # Create a ref to the input\n",
        "    input_copy = input\n",
        "\n",
        "    if (padding != 0):\n",
        "        input_copy = np.pad(input_copy, \n",
        "                               pad_width=((0,), (0,), (padding,), (padding,)), mode='constant', constant_values=(0.,))\n",
        "\n",
        "    batch_str, channel_str, kern_h_str, kern_w_str = input_copy.strides\n",
        "    # Get the h and w of the convolution output \n",
        "    out_h = conv_dim[0]\n",
        "    out_w = conv_dim[1]\n",
        "\n",
        "    # Unpack \n",
        "    out_batch, out_channels, _, _ = input_copy.shape\n",
        "    \n",
        "    return np.lib.stride_tricks.as_strided(\n",
        "        input_copy,\n",
        "        (out_batch, out_channels, out_h, out_w, kernel_size, kernel_size),\n",
        "        (batch_str, channel_str, kern_h_str, kern_w_str, kern_h_str, kern_w_str)\n",
        "    )\n",
        "\n",
        "def fetch_raw_windows(in_x, win_dims, strides):\n",
        "    win = np.lib.stride_tricks.as_strided(in_x, win_dims, strides)\n",
        "    return win"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ufVGaMX6-YY"
      },
      "outputs": [],
      "source": [
        "###############################################################\n",
        "## Implementation based on Vectorized CNN \n",
        "## Using NP Stride Tricks and EinSum \n",
        "## https://jessicastringham.net/2017/12/31/stride-tricks/\n",
        "## https://ajcr.net/Basic-guide-to-einsum/\n",
        "###############################################################\n",
        "\n",
        "class ConvolutionLayer:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        # self.input_h = in_h\n",
        "        # self.input_w = in_w\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Xavier Initialization \n",
        "        self._init_weights_bias()\n",
        "        self.input = None\n",
        "        self.windows = None\n",
        "        self.grads = None\n",
        "        self.id = 0\n",
        "        # Adam optimizer params\n",
        "        self.m_dw = 0\n",
        "        self.m_db = 0\n",
        "        self.v_dw = 0\n",
        "        self.v_db = 0 \n",
        "\n",
        "    def _init_weights_bias(self):\n",
        "        # self.kernel = np.random.normal(0, 0.001, size=(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        # self.kernel = np.ones(shape=(out_channels, in_channels, kernel_size, kernel_size))*scale\n",
        "        ## XAVIER INITIALIZATION\n",
        "        limit = np.sqrt(2/(self.in_channels+self.out_channels))\n",
        "        self.kernel = np.random.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)*limit*scale\n",
        "        self.bias = np.zeros(self.out_channels)\n",
        "\n",
        "    def forward(self, x: np.ndarray):\n",
        "        \n",
        "        ## Forward Pass\n",
        "        ## Assume input x is of the form:\n",
        "        ## x : (B, c_in, h, w)\n",
        "        ## We want to convolve X and kernel\n",
        "        ## Create windows from X of the shape of the kernel\n",
        "\n",
        "        b, c, in_h, in_w = x.shape\n",
        "\n",
        "        self.output_h = in_h - self.kernel_size + 1\n",
        "        self.output_w = in_w - self.kernel_size + 1\n",
        "\n",
        "        windows = fetch_windows(x, (self.output_h, self.output_w), self.kernel_size)\n",
        "\n",
        "        # b = batch size \n",
        "        # n = num of output channels \n",
        "        # i = num input channels \n",
        "        # h,w = output height, weight dim\n",
        "        # j, k = kernel dimensions \n",
        "\n",
        "        # Output eq\n",
        "        \n",
        "        output = np.einsum('bihwjk,nijk->bnhw', windows, self.kernel, optimize=True)\n",
        "        \n",
        "        output += self.bias[None, :, None, None]\n",
        "\n",
        "        # We get an output of size (batch x output channels x h x w) as reqd \n",
        "        \n",
        "        # Output: (B, c_out, output_h, output_w)\n",
        "        self.input = x\n",
        "        self.windows = windows\n",
        "\n",
        "        return output\n",
        "    \n",
        "    def backward(self, dout, alpha = 0.001):\n",
        "        # t1 = time.time()\n",
        "        # print(\"Conv back, shape:\", dout.shape)\n",
        "        # Get saved x and windows\n",
        "        x = self.input\n",
        "        windows = self.windows\n",
        "\n",
        "        # print(\"Conv back, windows shape:\", windows.shape)\n",
        "\n",
        "        # n = num of output channels \n",
        "        # i = num input channels \n",
        "        # j,k = kernel dimensions \n",
        "        # b,c = dimensions of n_windows arr\n",
        "\n",
        "        # Gradient w.r.t. k\n",
        "        dk = np.einsum('bihwjk,bnhw->nijk',windows,dout,optimize=True)\n",
        "\n",
        "        # Gradient w.r.t. bias\n",
        "        db = np.sum(dout, axis=(0,2,3))\n",
        "\n",
        "        # Gradient w.r.t. x\n",
        "        pad = self.kernel_size - 1\n",
        "        dout_windows = fetch_windows(dout, (x.shape[2], x.shape[3]), self.kernel_size, padding=pad)\n",
        "\n",
        "        # Flip the kernel 180 degrees \n",
        "        flip = np.rot90(self.kernel, 2, axes=(2, 3))\n",
        "        \n",
        "        # Gradient w.r.t. input (x)\n",
        "        # CHECK THIS! \n",
        "        dx = np.einsum('bnhwjk,nijk->bihw', dout_windows, flip, optimize=True)\n",
        "\n",
        "        # print(\"Grad shape:\", dx.shape)\n",
        "\n",
        "        # dx = np.einsum('nijk,abcijk->ibc',kernel_rot, dout_windows)\n",
        "\n",
        "        self.grads = (dk,db)\n",
        "\n",
        "        # Return the gradients\n",
        "        # To be used by the Adam Optimizer\n",
        "        # t2 = time.time()\n",
        "        # print(\"Time for back pas:\", t2-t1)\n",
        "        return dx\n",
        "\n",
        "    def update(self, beta1, beta2, epsilon, alpha, t):\n",
        "        #beta1 : exp decay parameter for 1st moment \n",
        "        #beta2 : exp decay parameter for 2nd moment \n",
        "        dw = self.grads[0]\n",
        "        db = self.grads[1]\n",
        "\n",
        "        # Update 1st moment estimates\n",
        "        self.m_dw = beta1*self.m_dw + (1-beta1)*dw\n",
        "        self.m_db = beta1*self.m_db + (1-beta1)*db\n",
        "\n",
        "        # Update 2nd moment estimates\n",
        "        self.v_dw = beta2*self.v_dw + (1-beta2)*(dw**2)\n",
        "        self.v_db = beta2*self.v_db + (1-beta2)*(db**2)\n",
        "\n",
        "        ## bias correction\n",
        "        m_dw_corr = self.m_dw/(1-beta1**t)\n",
        "        m_db_corr = self.m_db/(1-beta1**t)\n",
        "        v_dw_corr = self.v_dw/(1-beta2**t)\n",
        "        v_db_corr = self.v_db/(1-beta2**t)\n",
        "\n",
        "        ## update w and biases\n",
        "        self.kernel = self.kernel - alpha*(m_dw_corr/(np.sqrt(v_dw_corr)+epsilon))\n",
        "        self.bias = self.bias - alpha*(m_db_corr/(np.sqrt(v_db_corr)+epsilon))\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmZeapjbpMjM"
      },
      "outputs": [],
      "source": [
        "#import numpy as np\n",
        "\n",
        "class FCLayer:\n",
        "    def __init__(self, nrows, ncols):\n",
        "        self.nrows = nrows \n",
        "        self.ncols = ncols\n",
        "        self.input = None\n",
        "        self._init_weights_bias()\n",
        "        self.grads = None\n",
        "        self.id = 1\n",
        "        # Adam optimizer params\n",
        "        self.m_dw = 0\n",
        "        self.m_db = 0\n",
        "        self.v_dw = 0\n",
        "        self.v_db = 0 \n",
        "\n",
        "    def _init_weights_bias(self):\n",
        "        f_in = self.nrows\n",
        "        f_out = self.ncols\n",
        "        limit = np.sqrt(2/float(f_in+f_out))\n",
        "\n",
        "        # He Initialization of weights and bias\n",
        "        self.weights = np.random.randn(f_in, f_out)*limit*scale\n",
        "        # self.weights = np.random.randn(-1*limit, limit, size=(f_in, f_out))*scale\n",
        "        # self.weights = np.random.normal(0.0, limit, size=(f_in, f_out))\n",
        "        # self.weights = np.random.rand*scale\n",
        "        # self.weights = np.random.normal(0, 0.001, size=(f_in,f_out))\n",
        "\n",
        "        # print(self.weights[0])\n",
        "        self.bias = np.zeros(f_out)\n",
        "\n",
        "    def forward(self, fc: np.ndarray):\n",
        "        ## Forward Pass\n",
        "        ## Assume input x is of the form:``\n",
        "        ## x : (B x nrows)\n",
        "\n",
        "        # Calculate the dot product \n",
        "        out = np.dot(fc, self.weights) + self.bias\n",
        "\n",
        "        # Save the input for backprop\n",
        "        self.input = fc\n",
        "\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout, alpha = 0.001):\n",
        "\n",
        "        fc = self.input\n",
        "\n",
        "        # Get the batch size \n",
        "        m = fc.shape[0]\n",
        "\n",
        "        # Weight gradient\n",
        "        dw = np.dot(fc.T, dout)\n",
        "\n",
        "        # Bias gradient \n",
        "        db = np.sum(dout, axis = 0)\n",
        "    \n",
        "        # Input gradient \n",
        "        dx = np.dot(dout, self.weights.T) \n",
        "\n",
        "        self.grads = (dw, db)\n",
        "        \n",
        "        return dx\n",
        "\n",
        "    def update(self, beta1, beta2, epsilon, alpha, t):\n",
        "        #beta1 : exp decay parameter for 1st moment \n",
        "        #beta2 : exp decay parameter for 2nd moment \n",
        "        dw = self.grads[0]\n",
        "        db = self.grads[1]\n",
        "\n",
        "        # Update 1st moment estimates\n",
        "        self.m_dw = beta1*self.m_dw + (1-beta1)*dw\n",
        "        self.m_db = beta1*self.m_db + (1-beta1)*db\n",
        "\n",
        "        # Update 2nd moment estimates\n",
        "        self.v_dw = beta2*self.v_dw + (1-beta2)*(dw**2)\n",
        "        self.v_db = beta2*self.v_db + (1-beta2)*(db**2)\n",
        "\n",
        "        ## bias correction\n",
        "        m_dw_corr = self.m_dw/(1-beta1**t)\n",
        "        m_db_corr = self.m_db/(1-beta1**t)\n",
        "        v_dw_corr = self.v_dw/(1-beta2**t)\n",
        "        v_db_corr = self.v_db/(1-beta2**t)\n",
        "\n",
        "        ## update w and biases\n",
        "        self.weights = self.weights - alpha*(m_dw_corr/(np.sqrt(v_dw_corr)+epsilon))\n",
        "        self.bias = self.bias - alpha*(m_db_corr/(np.sqrt(v_db_corr)+epsilon))\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MaxPool:\n",
        "    def __init__(self, pool_size, stride=1, padding=0):\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.input = None\n",
        "        self.windows = None\n",
        "        self.indices = None\n",
        "        self.in_shape = None\n",
        "        self.out_shape = None\n",
        "        self.grads = None\n",
        "        self.id = 2\n",
        "\n",
        "    def forward(self, x : np.ndarray):\n",
        "        # Input is of the format (batch, c, w, h)\n",
        "\n",
        "        b, c, h, w, = x.shape\n",
        "\n",
        "        # stride = self.stride\n",
        "        pool = self.pool_size\n",
        "\n",
        "        # mat_pad = x[:, :, : (h - pool) // stride * stride + pool, : (w - pool) // stride * stride + pool]\n",
        "\n",
        "        out_h = 1 + ((h - pool)//pool)\n",
        "        out_w = 1 + ((w - pool)//pool)\n",
        "\n",
        "        b_str, ch_str, k_h_str, k_w_str = x.strides\n",
        "\n",
        "        # View is of the form (batch, c, out_w, out_h, pool, pool)\n",
        "        view = fetch_raw_windows(x, (b, c, out_h, out_w, pool, pool), \\\n",
        "          (x.strides[0], x.strides[1], x.strides[2]*pool, x.strides[3]*pool,x.strides[2], x.strides[3]))\n",
        "\n",
        "        # Store indices of max elements \n",
        "        new_shape = view.shape[:4] + (pool * pool, )\n",
        "        tmp = view.reshape(new_shape)\n",
        "        self.indices = np.argmax(view.reshape(new_shape), axis=4)\n",
        "        out = np.max(tmp, axis=4)\n",
        "\n",
        "        self.in_shape = x.shape\n",
        "        self.out_shape = out.shape\n",
        "       \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout : np.ndarray):\n",
        "        # Input gradients \n",
        "        dx = np.zeros(self.in_shape)\n",
        "\n",
        "        # Get the dimensions of the output\n",
        "        b, c, out_h, out_w = self.out_shape\n",
        "        pool = self.pool_size\n",
        "\n",
        "        # Get a view of the windows \n",
        "        view = fetch_raw_windows(dx, (b, c, out_h, out_w, pool, pool), \\\n",
        "          (dx.strides[0], dx.strides[1], dx.strides[2]*pool, dx.strides[3]*pool,dx.strides[2], dx.strides[3]))\n",
        "\n",
        "        # Set the values in the view \n",
        "        for b1 in range(b):\n",
        "            for c1 in range(c):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        tmp = self.indices[b1, c1, i, j]\n",
        "                        t1 = (int)(tmp // pool)\n",
        "                        t2 = (int)(tmp % pool)\n",
        "                        view[b1, c1, i, j, t1, t2] += dout[b1, c1, i, j]\n",
        "\n",
        "        return dx\n",
        "\n",
        "    def update(self, beta1, beta2, epsilon, alpha, t):\n",
        "        return"
      ],
      "metadata": {
        "id": "yJK_GEQuJaEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvBJCrx972x-"
      },
      "outputs": [],
      "source": [
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.grads = None\n",
        "        self.id = 3\n",
        "        return\n",
        "\n",
        "    def forward(self, x : np.ndarray):\n",
        "        z = np.where(x > 0, x, 0)                          \n",
        "        self.input = x\n",
        "        return z\n",
        "\n",
        "    def backward(self, dout : np.ndarray):\n",
        "        x = self.input\n",
        "        # mask = np.ones_like(dout)\n",
        "        # mask[x<=0] = 0.01\n",
        "        dx = np.array(dout, copy = True)\n",
        "  \n",
        "        dx[x <= 0] = 0\n",
        "        return dx\n",
        "\n",
        "    def update(self, beta1, beta2, epsilon, alpha, t):\n",
        "      return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReShape:\n",
        "    def __init__(self, in_shape, out_shape):\n",
        "        self.input = None\n",
        "        self.in_shape = in_shape\n",
        "        self.out_shape = out_shape\n",
        "        self.id=4\n",
        "        self.grads = None\n",
        "      \n",
        "    def forward(self, x: np.ndarray):\n",
        "        # print(x.shape)\n",
        "        # print(self.in_shape)\n",
        "        assert(x.shape == self.in_shape)\n",
        "        y = np.reshape(x,self.out_shape)\n",
        "        return y\n",
        "\n",
        "    def backward(self, dout):\n",
        "        assert (dout.shape == self.out_shape)\n",
        "        retval = np.reshape(dout, self.in_shape)\n",
        "        return retval\n",
        "\n",
        "    def update(self, beta1, beta2, epsilon, alpha, t):\n",
        "      return"
      ],
      "metadata": {
        "id": "Z8tS8Zj7tWIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cce_loss(logits, labels):\n",
        "    # logits = output of the last fc layer \n",
        "    temp= np.array(logits, copy=True)\n",
        "    temp -= np.max(temp, axis=1, keepdims=True)\n",
        "    softmax_probs = np.exp(temp) / np.sum(np.exp(temp), axis=1, keepdims=True)\n",
        "\n",
        "    # softmax_probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
        "\n",
        "    n = logits.shape[0]\n",
        "\n",
        "    # Convert targets to one-hot encoding\n",
        "    labels_one_hot = np.zeros_like(logits)\n",
        "    labels_one_hot[np.arange(n), labels] = 1\n",
        "\n",
        "    # Compute loss\n",
        "    loss = -(1/n) * np.sum(labels_one_hot * np.log(softmax_probs + 1e-6))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "UaX74Z47YRb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "    softmax_probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
        "    return softmax_probs\n",
        "\n",
        "def grad(logits, labels):\n",
        "    # Assume targets are one hot encoded\n",
        "    temp= np.array(logits, copy=True)\n",
        "    temp -= np.max(temp, axis=1, keepdims=True)\n",
        "    softmax_probs = np.exp(temp) / np.sum(np.exp(temp), axis=1, keepdims=True)\n",
        "    \n",
        "    # softmax_probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
        "\n",
        "    n = logits.shape[0]\n",
        "    # Convert to one hot encoded \n",
        "    targets = np.zeros_like(logits)\n",
        "    targets[np.arange(n), labels] = 1\n",
        "\n",
        "    # Compute the softmax probabilities\n",
        "    # Compute the gradient of the cross-entropy loss w.r.t. the logits\n",
        "    n = logits.shape[0]\n",
        "    grad = (softmax_probs - targets) \n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "gjwqKcLwfVC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0NdTaMXC6fn",
        "outputId": "95132784-92df-4c88-92d0-5ce7f724bcb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the transformation to be applied to the dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Load the CIFAR10 dataset and apply the defined transformation\n",
        "train_set = datasets.CIFAR10('./data', train=True, download=True,transform=transform)\n",
        "val_set = datasets.CIFAR10('./data', train=False, download=True,transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N190KECJWQuQ"
      },
      "outputs": [],
      "source": [
        "# Convert the PyTorch dataset to a numpy array\n",
        "train_data = np.array(train_set.data)\n",
        "train_labels = np.array(train_set.targets)\n",
        "\n",
        "val_data = np.array(val_set.data)\n",
        "val_labels = np.array(val_set.targets)\n",
        "\n",
        "train_data = np.moveaxis(train_data, -1, 1)\n",
        "val_data = np.moveaxis(val_data, -1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(inp):\n",
        "    n_channels = inp.shape[1]\n",
        "    \n",
        "    # Calculate the mean and standard deviation of each channel of the batch\n",
        "    mean = np.mean(inp, axis=(0, 2, 3))\n",
        "    std = np.std(inp, axis=(0, 2, 3))\n",
        "    \n",
        "    # Normalize the batch using the mean and standard deviation\n",
        "    normalized = (inp - mean.reshape(1, n_channels, 1, 1)) / (std.reshape(1, n_channels, 1, 1)+1e-20)\n",
        "    \n",
        "    return normalized"
      ],
      "metadata": {
        "id": "bDvPRaddbm8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = normalize(train_data)"
      ],
      "metadata": {
        "id": "8fXgDG1jbrhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the layers of the model \n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "Conv1 = ConvolutionLayer(in_channels=3, out_channels=32,kernel_size=3)\n",
        "\n",
        "MP1 = MaxPool(2)\n",
        "\n",
        "Conv2 = ConvolutionLayer(in_channels=32,out_channels=64,kernel_size=5)\n",
        "\n",
        "MP2 = MaxPool(2)\n",
        "\n",
        "Conv3 = ConvolutionLayer(in_channels=64,out_channels=64,kernel_size=3)\n",
        "\n",
        "FC1 = FCLayer(64*3*3, 64)\n",
        "FC2 = FCLayer(64,10)\n",
        "\n",
        "relu1 = ReLU()\n",
        "relu2 = ReLU()\n",
        "relu3 = ReLU()\n",
        "relu4 = ReLU()\n",
        "relu5 = ReLU()\n",
        "\n",
        "RS  = ReShape((batch_size, 64, 3, 3), (batch_size, 3*3*64))\n",
        "\n",
        "layers = [Conv1, relu1, MP1, Conv2, relu2, MP2, Conv3, relu3, RS, FC1, relu4, FC2]"
      ],
      "metadata": {
        "id": "IXt22WzAU4pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ADAM OPTIMIZER\n",
        "import time \n",
        "\n",
        "# Iteration count \n",
        "t = 1\n",
        "\n",
        "n_epochs = 20\n",
        "# train_size = 512\n",
        "train_size = train_data.shape[0]\n",
        "# train_size = 64\n",
        "\n",
        "batch_idx = 0\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    print(\"Performing epoch: \", i+1)\n",
        "    print(\"---------------------------\")\n",
        "    \n",
        "    iter = 0\n",
        "    while(batch_idx < train_size):\n",
        "        iter += 1\n",
        "        train_batch = train_data[batch_idx:min(batch_idx+batch_size, train_size)]\n",
        "        train_batch_labels = train_labels[batch_idx:min(batch_idx+batch_size, train_size)]\n",
        "        out = train_batch\n",
        "\n",
        "        # Forward Pass  \n",
        "        for l in layers:\n",
        "          out = l.forward(out)\n",
        "      \n",
        "        loss = cce_loss(out, train_batch_labels)\n",
        "        if (iter % 500 == 0):\n",
        "          print(\"[ITER {}] Loss:\".format(iter), loss)\n",
        "\n",
        "        # Backward Pass\n",
        "        inp = grad(out, train_batch_labels)\n",
        "\n",
        "        for l in reversed(layers):\n",
        "          # print(inp[0])\n",
        "          inp = l.backward(inp)\n",
        "        \n",
        "        # Now all the gradients are set\n",
        "        # Update\n",
        "        for l in layers:\n",
        "          l.update(0.9,0.999,1e-8,0.001,t)\n",
        "        \n",
        "        batch_idx += batch_size\n",
        "        t += 1\n",
        "\n",
        "    # Calculate Validation Accuracy \n",
        "    # val_idx = 0\n",
        "    # # val_size = val_data.shape[0]\n",
        "    # val_size = 64\n",
        "    # correct = 0\n",
        "    # total = 0\n",
        "    # classes = [0 for _ in range(10)]\n",
        "    # classes_total = [0 for _ in range(10)]\n",
        "\n",
        "    # while (val_idx < val_size):\n",
        "    #   val_batch = val_data[val_idx:min(val_idx+batch_size, val_size)]\n",
        "    #   val_batch_labels = val_labels[val_idx:min(val_idx+batch_size, val_size)]\n",
        "\n",
        "    #   out = val_batch\n",
        "\n",
        "    #   # Forward Pass  \n",
        "    #   for l in layers:\n",
        "    #     out = l.forward(out)\n",
        "\n",
        "    #   out = softmax(out)\n",
        "\n",
        "    #   predictions = np.argmax(out, axis=1)\n",
        "    #   # print(predictions.shape)\n",
        "    #   # print(val_batch_labels.shape)\n",
        "    #   for k in range(len(predictions)):\n",
        "    #     total+=1 \n",
        "    #     classes_total[predictions[k]] += 1\n",
        "    #     if (predictions[k]==val_batch_labels[k]):\n",
        "    #       correct += 1\n",
        "    #       classes[predictions[k]] += 1\n",
        "        \n",
        "    #   # correct += np.sum(predictions == val_batch_labels)\n",
        "    #   # total += batch_size \n",
        "\n",
        "    #   val_idx += batch_size\n",
        "    \n",
        "    # print(\"Validation Accuracy after Epoch {} is:\".format(i), correct * 100 / total)\n",
        "    # print(\"Class wise scores:\", classes, classes_total)\n",
        "    batch_idx = 0"
      ],
      "metadata": {
        "id": "qp2lJEqbG7s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Optimizer Pass\n",
        "# # - - - - - - - - - - - - \n",
        "# import time \n",
        "\n",
        "# n_epochs = 2000\n",
        "# batch_idx = 0\n",
        "\n",
        "# # train_size = 32\n",
        "# # train_size = 512\n",
        "# train_size = train_data.shape[0]\n",
        "\n",
        "\n",
        "# for i in range(n_epochs):\n",
        "#   print(\"Performing epoch\", i)\n",
        "#   iter = 0\n",
        "#   while(batch_idx < train_size):\n",
        "#     # t1 = time.time()\n",
        "#     iter += 1\n",
        "#     # Select Batch \n",
        "#     train_batch = train_data[batch_idx:min(batch_idx+batch_size, train_size)]\n",
        "#     train_batch_labels = train_labels[batch_idx:min(batch_idx+batch_size, train_size)]\n",
        "\n",
        "#     out = train_batch\n",
        "\n",
        "#     # Forward Pass \n",
        "#     # t1 = time.time()\n",
        "#     # t1 = time.time()\n",
        "#     for l in layers:\n",
        "#       out = l.forward(out)\n",
        "#     # t2 = time.time()\n",
        "#     # print(\"Total time for fwd pas:\", t2-t1)\n",
        "#       # print(\"OUT:\", out[0])\n",
        "#     # t2 = time.time()\n",
        "#     # print(\"Fwd time for one iter:\", t2-t1)\n",
        "#     # print(\"=================================\")\n",
        "  \n",
        "#     loss = cce_loss(out, train_batch_labels)\n",
        "    \n",
        "#     # print(\"Curr time is:\", t)\n",
        "#     if (iter % 100 ):\n",
        "#       print(\"Iter: \", iter, \" batch_idx:\", batch_idx)\n",
        "#       print(\"Loss is \", loss)\n",
        "#     # Backward Pass \n",
        "#     inp = grad(out, train_batch_labels)\n",
        "#     # print(\"============================\")\n",
        "#     # print(inp)\n",
        "#     # print(\"============================\")\n",
        "#     # t1 = time.time()\n",
        "#     # t1 = time.time()\n",
        "#     for l in reversed(layers):\n",
        "#       inp = l.backward(inp)\n",
        "#       # print(inp)\n",
        "#     # t2 = time.time()\n",
        "#     # print(\"Total time for back pas:\", t2-t1)\n",
        "#       # if (iter%100):\n",
        "#       #  print(\"Gradient:\",inp[0])\n",
        "#       # print(\"Back gradient shape:\", inp.shape)\n",
        "#     # t2 = time.time()\n",
        "#     # print(\"Back time for one iter:\", t2-t1)\n",
        "#     # time.sleep\n",
        "#     # Now all the gradients are set\n",
        "#     # Update weights etc.\n",
        "#     alpha = 0.001\n",
        "#     for l in layers:\n",
        "#       if (l.id == 0):\n",
        "#         l.kernel -= alpha* l.grads[0]\n",
        "#         l.bias -= alpha* l.grads[1]\n",
        "#       elif (l.id == 1):\n",
        "#         l.weights -= alpha* l.grads[0]\n",
        "#         l.bias -= alpha*l.grads[1]\n",
        "\n",
        "#     # Completed, increase idx\n",
        "#     batch_idx += batch_size\n",
        "#     # t2 = time.time()\n",
        "#     # print(\"Time for one iteration:\", t2-t1)\n",
        "\n",
        "#   print(\"Completed epoch\", i)\n",
        "#   batch_idx = 0"
      ],
      "metadata": {
        "id": "qz_-oNh-0_xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the layers of the model \n",
        "\n",
        "# batch_size = 32\n",
        "\n",
        "# FC1 = FCLayer(64*22*22, 64)\n",
        "# FC2 = FCLayer(64, 10)\n",
        "\n",
        "# Conv1 = ConvolutionLayer(3, 32, 3)\n",
        "# Conv2 = ConvolutionLayer(32, 64, 5)\n",
        "# Conv3 = ConvolutionLayer(64, 64, 3)\n",
        "\n",
        "# relu1 = ReLU()\n",
        "# relu2 = ReLU()\n",
        "# relu3 = ReLU()\n",
        "# relu4 = ReLU()\n",
        "# relu5 = ReLU()\n",
        "# MP1 = MaxPool(2)\n",
        "# MP2 = MaxPool(2)\n",
        "# RS  = ReShape((batch_size, 64, 22, 22), (batch_size, 22*22*64))\n",
        "\n",
        "\n",
        "# layers = [Conv1, relu1, MP1, Conv2, relu2, MP2, Conv3, relu3, RS, FC1, relu4, FC2]"
      ],
      "metadata": {
        "id": "WCswRXReLfY9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}